{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aefb204",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Jon Ball\"\n",
    "__version__ = \"Summer 2022\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a39b5",
   "metadata": {},
   "source": [
    "Code adapted from Muhammad Haseeb Khan and Adji Dieng's sample script:\n",
    "https://github.com/adjidieng/ETM/blob/master/scripts/data_nyt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b384cdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.7\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6bf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from scipy import sparse\n",
    "\n",
    "import numpy as np \n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd393f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b28cc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for saving vocab, embeddings, etc.\n",
    "save_path = os.path.join(os.getcwd(), \"soced_min_df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57501fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation and misc. digits to add to stop word filter\n",
    "punct = [punctuation[idx] for idx in range(len(punctuation))]\n",
    "digits = [str(idx) for idx in range(1000)] + [\"00\", \"000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacb544b",
   "metadata": {},
   "source": [
    "https://github.com/adjidieng/ETM/blob/master/scripts/stops.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e17ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stops.txt\", \"r\") as infile:\n",
    "    stops = [s.rstrip() for s in infile.readlines()]\n",
    "    stops += punctuation\n",
    "    stops += digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9442c9",
   "metadata": {},
   "source": [
    "https://github.com/adjidieng/ETM/blob/master/scripts/data_nyt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1018982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.7\n",
    "min_df = 2 # min_df set low due to small sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e547a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5722 docs loaded for Soc of Ed. \n",
      "Each doc is a title or sentence drawn from an article written by a sociologist of education.\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"eric_data\", \"soced_etm_inputs.txt\"), \"r\") as infile:\n",
    "    docs = [line.lower().rstrip() for line in infile.readlines()]\n",
    "print(f\"\"\"{len(docs)} docs loaded for Soc of Ed. \n",
    "Each doc is a title or sentence drawn from an article written by a sociologist of education.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4297f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender differences in context: the impact of track position on study involvement in flemish secondary education.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6da93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tfidf vectorizer\n",
    "tfidf_vec = TfidfVectorizer(min_df=min_df, max_df=max_df, stop_words=stops)\n",
    "tfidf_matrix = tfidf_vec.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3055c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4364 word types in the Soc of Ed text sample.\n",
      "  Initial vocabulary size: 4364\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "vocab = tfidf_vec.get_feature_names()\n",
    "print(f\"{len(vocab)} word types in the Soc of Ed text sample.\")\n",
    "print(f\"  Initial vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7edd4bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents into train/test/valid...\n"
     ]
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print(\"Splitting documents into train/test/valid...\")\n",
    "n_docs = tfidf_matrix.shape[0]\n",
    "trainSize = int(np.floor(0.85 * n_docs))\n",
    "testSize = int(np.floor(0.10 * n_docs))\n",
    "valSize = int(n_docs - trainSize - testSize)\n",
    "idx_permute = np.random.permutation(n_docs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4d3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = range(trainSize)\n",
    "test_idx = range(trainSize, trainSize + testSize)\n",
    "val_idx = range(trainSize + testSize, trainSize + testSize + valSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42dc8c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Vocabulary after removing words not in train data: 4288\n"
     ]
    }
   ],
   "source": [
    "# Remove word types not in train data and map vocab to indices\n",
    "vocab = sorted(list(set(\n",
    "        [w for idx in train_idx for w in word_tokenize(docs[idx_permute[idx]]) if w in vocab]\n",
    ")))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print(f\"   Vocabulary after removing words not in train data: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea5d6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocab\n",
    "with open(os.path.join(save_path, \"vocab.pkl\"), \"wb\") as outfile:\n",
    "    pickle.dump(vocab, outfile)\n",
    "# Save word-to-index mapping\n",
    "with open(os.path.join(save_path, \"word2id.pkl\"), \"wb\") as outfile:\n",
    "    pickle.dump(word2id, outfile)\n",
    "# Save index-to-word mapping\n",
    "with open(os.path.join(save_path, \"id2word.pkl\"), \"wb\") as outfile:\n",
    "    pickle.dump(id2word, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4b9a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = [\n",
    "    [word2id[w] for w in word_tokenize(docs[idx_permute[idx]]) if w in word2id] for idx in train_idx\n",
    "]\n",
    "docs_test = [\n",
    "    [word2id[w] for w in word_tokenize(docs[idx_permute[idx]]) if w in word2id] for idx in test_idx\n",
    "]\n",
    "docs_val = [\n",
    "    [word2id[w] for w in word_tokenize(docs[idx_permute[idx]]) if w in word2id] for idx in val_idx\n",
    "]\n",
    "del docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2eeb36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number of documents (train): 4863 [this should be equal to 4863]\n",
      "   Number of documents (test): 572 [this should be equal to 572]\n",
      "   Number of documents (valid): 287 [this should be equal to 287]\n"
     ]
    }
   ],
   "source": [
    "print(f\"   Number of documents (train): {len(docs_train)} [this should be equal to {trainSize}]\")\n",
    "print(f\"   Number of documents (test): {len(docs_test)} [this should be equal to {testSize}]\")\n",
    "print(f\"   Number of documents (valid): {len(docs_val)} [this should be equal to {valSize}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c61eda46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing empty documents...\n"
     ]
    }
   ],
   "source": [
    "# Remove empty documents\n",
    "print(\"Removing empty documents...\")\n",
    "\n",
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "docs_train = remove_empty(docs_train)\n",
    "docs_test = remove_empty(docs_test)\n",
    "docs_val = remove_empty(docs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d26672c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test documents with length=1\n",
    "docs_test = [doc for doc in docs_test if len(doc)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdeebd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting test documents in 2 halves...\n"
     ]
    }
   ],
   "source": [
    "# Split test set in 2 halves\n",
    "print(\"Splitting test documents in 2 halves...\")\n",
    "docs_test_h1 = [[w for i,w in enumerate(doc) if i <= len(doc) / 2.0-1] for doc in docs_test]\n",
    "docs_test_h2 = [[w for i,w in enumerate(doc) if i > len(doc) / 2.0-1] for doc in docs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e83446",
   "metadata": {},
   "source": [
    "Added step: load embeddings and map to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd6e89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 2.26 s, total: 40.7 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load pre-trained ERIC word embeddings, map to index, and store in matrix format\n",
    "embeddings = np.zeros((len(vocab), 300)) # Vocab size x embedding size\n",
    "\n",
    "eric_embeds = {}\n",
    "with open(os.path.join(\"eric_data\", \"eric_embeds_50.txt\"), \"r\") as infile:\n",
    "    for line in infile.readlines():\n",
    "        e = line.split()\n",
    "        eric_embeds[e[0]] = np.array(e[1:])\n",
    "        \n",
    "for embed in eric_embeds:\n",
    "    if embed in vocab:\n",
    "        embeddings[word2id[embed],] = eric_embeds[embed]\n",
    "\n",
    "with open(os.path.join(save_path, \"embeddings.npy\"), \"wb\") as outfile:\n",
    "    np.save(outfile, embeddings)\n",
    "\n",
    "del eric_embeds, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cca10e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lists of words...\n",
      "  len(words_train):  55782\n",
      "  len(words_test):  6404\n",
      "  len(words_test_h1):  3067\n",
      "  len(words_test_h2):  3337\n",
      "  len(words_val):  3252\n"
     ]
    }
   ],
   "source": [
    "# Getting lists of words and doc_indices\n",
    "print(\"Creating lists of words...\")\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_train = create_list_words(docs_train)\n",
    "words_test = create_list_words(docs_test)\n",
    "words_test_h1 = create_list_words(docs_test_h1)\n",
    "words_test_h2 = create_list_words(docs_test_h2)\n",
    "words_val = create_list_words(docs_val)\n",
    "\n",
    "print(\"  len(words_train): \", len(words_train))\n",
    "print(\"  len(words_test): \", len(words_test))\n",
    "print(\"  len(words_test_h1): \", len(words_test_h1))\n",
    "print(\"  len(words_test_h2): \", len(words_test_h2))\n",
    "print(\"  len(words_val): \", len(words_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e68d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting doc indices...\n",
      "  len(np.unique(doc_indices_train)): 4854 [this should be 4854]\n",
      "  len(np.unique(doc_indices_test)): 565 [this should be 565]\n",
      "  len(np.unique(doc_indices_test_h1)): 565 [this should be 565]\n",
      "  len(np.unique(doc_indices_test_h2)): 565 [this should be 565]\n",
      "  len(np.unique(doc_indices_val)): 286 [this should be 286]\n"
     ]
    }
   ],
   "source": [
    "# Get doc indices\n",
    "print(\"Getting doc indices...\")\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_train = create_doc_indices(docs_train)\n",
    "doc_indices_test = create_doc_indices(docs_test)\n",
    "doc_indices_test_h1 = create_doc_indices(docs_test_h1)\n",
    "doc_indices_test_h2 = create_doc_indices(docs_test_h2)\n",
    "doc_indices_val = create_doc_indices(docs_val)\n",
    "\n",
    "print(\"  len(np.unique(doc_indices_train)): {} [this should be {}]\".format(len(np.unique(doc_indices_train)), len(docs_train)))\n",
    "print(\"  len(np.unique(doc_indices_test)): {} [this should be {}]\".format(len(np.unique(doc_indices_test)), len(docs_test)))\n",
    "print(\"  len(np.unique(doc_indices_test_h1)): {} [this should be {}]\".format(len(np.unique(doc_indices_test_h1)), len(docs_test_h1)))\n",
    "print(\"  len(np.unique(doc_indices_test_h2)): {} [this should be {}]\".format(len(np.unique(doc_indices_test_h2)), len(docs_test_h2)))\n",
    "print(\"  len(np.unique(doc_indices_val)): {} [this should be {}]\".format(len(np.unique(doc_indices_val)), len(docs_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8c0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in each set\n",
    "n_docs_train = len(docs_train)\n",
    "n_docs_test = len(docs_test)\n",
    "n_docs_test_h1 = len(docs_test_h1)\n",
    "n_docs_test_h2 = len(docs_test_h2)\n",
    "n_docs_val = len(docs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4b680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused variables\n",
    "del docs_train\n",
    "del docs_test\n",
    "del docs_test_h1\n",
    "del docs_test_h2\n",
    "del docs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44941fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bow representation...\n"
     ]
    }
   ],
   "source": [
    "# Create bow representation\n",
    "print(\"Creating bow representation...\")\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, len(vocab))).tocsr()\n",
    "\n",
    "bow_train = create_bow(doc_indices_train, words_train, n_docs_train, len(vocab))\n",
    "bow_test = create_bow(doc_indices_test, words_test, n_docs_test, len(vocab))\n",
    "bow_test_h1 = create_bow(doc_indices_test_h1, words_test_h1, n_docs_test_h1, len(vocab))\n",
    "bow_test_h2 = create_bow(doc_indices_test_h2, words_test_h2, n_docs_test_h2, len(vocab))\n",
    "bow_val = create_bow(doc_indices_val, words_val, n_docs_val, len(vocab))\n",
    "\n",
    "del words_train\n",
    "del words_test\n",
    "del words_test_h1\n",
    "del words_test_h2\n",
    "del words_val\n",
    "del doc_indices_train\n",
    "del doc_indices_test\n",
    "del doc_indices_test_h1\n",
    "del doc_indices_test_h2\n",
    "del doc_indices_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01bcaec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready !!\n",
      "*************\n"
     ]
    }
   ],
   "source": [
    "# Save bow matrices\n",
    "# Train\n",
    "sparse.save_npz(os.path.join(save_path, \"bow_train.npz\"), bow_train)\n",
    "# Test\n",
    "sparse.save_npz(os.path.join(save_path, \"bow_test.npz\"), bow_test)\n",
    "# Test split 1\n",
    "sparse.save_npz(os.path.join(save_path, \"bow_test_h1.npz\"), bow_test_h1)\n",
    "# Test split 2\n",
    "sparse.save_npz(os.path.join(save_path, \"bow_test_h2.npz\"), bow_test_h2)\n",
    "# Val\n",
    "sparse.save_npz(os.path.join(save_path, \"bow_val\"), bow_val)\n",
    "\n",
    "print(\"Data ready !!\")\n",
    "print(\"*************\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ETM",
   "language": "python",
   "name": "etm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
